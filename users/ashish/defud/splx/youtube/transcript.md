# YouTube Transcript

**Source:** https://youtu.be/znrStq-RUT4?si=BfgA-4tF_Sw0fJOG

---

The Def podcast hosted by Chong Shu and Ashish Popley. >> Hello and welcome to the Def Podcast. In this episode, I have the pleasure of joining Sandy Dun and our new host Chung from Basis Adventures into this topic of AI security. Before we go further, I think it's important to introduce an illustrious career that Sandy has had in security with over 20 years in experience in security itself and more lately being a champion in OASP AI security chapters. She's done some phenomenal work on thinking about AI security, thinking about the threat landscape and currently working at a security vendor called SPLX which recently became very popular for their pioneering work in demonstrating how insecure Chad GPT5 was their latest release. I that thing caught my attention and then very recently I happened to be in a meetup where Sandy was also present. So I could not resist myself and ask her to let's jump on the show and try to talk about what she's doing and what I'm learning and so on and so forth. So welcome Sandy. >> Great to be here. Thanks for inviting me. >> Awesome. I've been following the lead quite a bit on AI security. And ultimately after some of back and forth in this topic, I'm like who's getting the real advantage here? The offense or the defense side? It's a bit of an elusive debate in my head. I do have a bias to think a particular way. So maybe we'll start with that. not so light question. Maybe you have thoughts on that, Sandy. >> Yeah, absolutely. So, I think initially that our adversaries have an advantage, but they've had an advantage for quite a while, you know, and the advantage that they have is speed and they don't have to to go present a new product or new initiative to the legal team and get buy off and they don't have to worry about governance and compliance. they can just go download the tool and the age-old fact that they we have to be right all of the time and they just have to be right once now. But I think that's temporary. I think in the long run with AI it's both it presents new challenges but I think long term there is the potential to be able to orchestrate it so we can finally get ahead of our adversaries and start putting some better coordination. I mean we have the resource advantage. We have things like OASP where we have people worldwide collaborating together and trying to come up with better practices sharing not only tools and resources but best practices and on how to deploy these things. So I think for for a while it'll get worse before it gets better. And I think the other thing that we we don't talk enough about is not just our endusers, but our consumers, our normal people that are out there just trying to use these devices. If you look at the FBI reports, I mean, a large a number of attacks are against our older population that are falling for fishing and and scams and things, and there's nobody stepping in to help them. I mean, for everyone who gets their identity stolen, I mean, what do you you get a HIPPA letter, right? Your identity is stolen. You got to have like who gets compensated for that? Who actually is protected and getting seeing any real protection for them losing their data and being compensated for that or being promised that they'll do better in the future other than this kind of Dale letter that we get with every one of these breaches. So, I think I think we have an interesting problem in front of us. It will get worse before it gets better, but I think the potential is there for it to get better as defenders. >> Awesome. I think yes, you're right. I think the jury is out on when it gets better for defense. But one thing is very clear, the consumption of AI on both sides is going to increase no matter what. They may increase at different pace, different times, but more AI is going to be consumed by both sides. And hopefully it's a good battle in that regard. >> It's a super interesting observation. I'm curious from your perspective, what are the most dangerous misconceptions about AI security that you encounter when working with enterprises? I the thing that I worry about the most is people underestimating the complexity of these systems that turning one knob, you know, like there's a trade-off between reliability and creativity or consistency even with bias. Like when you you try to improve one thing, it has this cascading because all of these vectors are interconnected and all of these weights rely on each other. You make one little change and it has this cascading effect. One of the experiments that I've run since the original release of Dolly is about every six months and with every new release I run an experiment. The first experiment I did was I asked it to create an image of a happy cyber security professor with a white horse and then I added one word cyber security into that dolly image generator and it the first image was this really cute nice looking professor and the as soon as I added the word cyber security she turned in she looked like a witch and I thought oh that's interesting that was my first indication of bias right that if they taught cyber security you were unattractive. And over time, I've continued to just run that same prompt to compare the results. And recently, it's very middle of the road. Both the cyber security professor and the regular professor, they're not attractive, they're not unattractive, they're not old, they're not young, they're it's right in the middle. And for me, as a a long time, you know, almost three years now of experimenting and playing with these things, I think, okay, is that the results I want? Do I want just average answers? is are we just trying to get the best most un you know not offensive you know answer you know is that the best outcome and so I I those are one the things that I think about a lot which is are we handing over our what is offensive or what isn't offensive what is attractive are we handing all of that control over to these big model makers to make those decisions for us even if you look at things like creating images. I'm sure you guys have went in to try to create an image and you get refused and you're not sure why. And it may be something I was trying to do one where it showed an experiment that it was a thought experiment about improving the safety around cars. And this gentleman said, "If you put a great big pointy spike in the middle of the steering wheel, it will significantly reduce the number of car crashes because you have this huge spike and people have this visual awareness that if they crash into something, they'll have the spike." And so I went in because I was doing a a presentation. I tried to create an image, got refused because big pointy spike in the middle of a steering wheel. It found it offensive. And so those are the things we're definitely at the frontier. You said that at the in previously. Hey, we know everybody's going to be using this. Now, we're still trying to figure out h how do we do these things? How do we make these things meet everyone's expectations around security and safety? >> Yeah. So maybe given a lot of hands-on work that you do with these things and especially at SPLX, what's latest top of your mind in terms of like things that you feel like are very researchy in nature and could take some time to become mainstream and things that are like there's no more research about this. This is happening live in in real environments like what's actually real versus yes, it's possible cost is too high. So there's really you could have theoretically great important harmful things but if the cost is too high nobody's doing it like so how do we separate these two segments where low cost high impact let's talk about that versus high cost high impact like what is low cost and high impact that you're observing >> I think your question is generated specifically on AI specific type attacks right Ashish and I will tell you I go out and look at the AI incident databases is and I look at the different research that's going on the research reports from open AAI from anthropic from Google as much as we see all of this research around the potential for prompt injection is a serious problem you know it absolutely is and it's and it's not there's no reliable way to solve it but are our adversaries necessarily going that direction yet limited cases right now and not because it's not available. You know, as soon as we tighten up everything else, I think they'll definitely they'll always go to what's the least amount of effort to get the result, right? And right now with the ability to do so much research, scanning, find, you'll see zero days. Now the number of days to exploit a zero day has went from it used to once a zero day was discovered. The time to patch you had a number of months before it was actively exploited. I think they have that down to like 24 hours now for some of these. I think that our adversaries right now are using them for recon. They're using them for vulnerability research. They're using them for code conversion. being able to actually code things at a low level that really took a level of expertise and knowledge that was pretty difficult to find. I think those advanced attacks are coming. They just don't have to go there right now. >> Yeah. So, I'm kind of in the same boat as you, Sandy. It just re rephrasing what I'm hearing from you, making sure we're sort of talking about the same thing. Attacking the model may seem sexy and lucrative, but using the model for doing traditional attacks is sexier in that sense. It's actually more >> easier. Yeah. Because it's low lower cost, high impact. And it kind of goes back to this phenomena of AI is here to help previous workflows, not create new workflows yet in the sense that my previous workflow was to recon, to exploit, to build, to generate code, and all of that good stuff. That sevenstep attack chain. what is the automation I can do in that step so I can compress my attack cycle and because it's an existing workflow we understand it >> versus when you're creating a model again to automate a workflow then you are trying to attack that workflow which was enabled by the model a very different story like I think the adoption of AI is not to the point where there's enough rich attack surface for attackers to go after attacking attack AI models like the time will come but today the AI is being used mostly to automate existing workflows which are traditional apps, traditional vulnerability management, traditional infrastructure security. I think that's where I feel the buck is and then maybe in that light what is it that we should be concerned about specifically? I think you mentioned zero days maybe a little bit more on that would help. What specific previous workflows that were hard are now being becoming very easy and what is the real evidence in terms of their usage? Where should we read about this? Where should we learn about this as audience here? >> Well, I think the evidence what we're seeing is if you look at the news for today, the two, you know, the big announcements were the npm attacks, the supply chain attacks, right? And then crowd strike. I mean, you know, here's a tool that is built for our defenses that was compromised in an npm attack. And so, um, supply chains like where and this is the age old problem, right? like the the Lego nature of all code which it's it's a whole bunch of packages you know that you remember trying to patch for log 4j when that came out like it wasn't like you just say push out one patch that went and updated all your log 4j instances what you found was then third and fourth layers level of code it was very difficult to find so as much as we provide guidance around and I've said it I wrote an article on it today asset asset management. Well, first you have to define, you know, asset is your entire supply chain. So, you know, how are how are you what's your process for sanitizing where people get code? How are you managing that for your developers? How are you making sure that they're downloading from the right sites with the right packages using the right things that have been scanned or the up-to-date versions that don't have any back doors in them? And then how are you tracking all of that? It's it's easy to say, but it's very difficult to do. >> What are the hardest things to get right when building a AI security posture management program? >> I think it it comes down to what are we trying to do? Almost have to take a step back. KPMG actually had some really good podcasts on the building the AI enabled business which is the one thing that I see frequently is people just kind of you know you have this really broken digital infrastructure that has all of this really bad stuff in it and then they throw AI on top of it. You know it's like no you know like that is like lighting a match in a gasoline factory. I mean it's you're only going to make things worse. So you really have to step take a step back and say okay what's our business objectives you know what are our business value workflows how have those been digitized you know do we have that have we enabled digital transformation and then how do we improve that process AI at one level it's just ones and zeros it's just software but the big difference between traditional cyber security is as As a CISO, you gave these binary numbers to saying we, you know, here's how many identities we have. Here's where we did went in and did our verified that people had the right access rights. This is how much SQL injection. Here's our website. Here's what we scanned. Here's what we found. Here's what we remediated. With AI, you know, you absolutely because the attack surface is so huge, you mathematically can't do all of the governance things. Like it's as much as people talk about being able to do AI governance, it's mathematically impossible to do it at the core level and understand absolutely everything and have the time and resources to be able to do that. It's mathematically impossible. So now you've moved into the statistical realm. Instead of having a binary discussion, you're really saying the percentage. And in reality, it it was a percentage conversation. We just weren't honest about it. Think about your third party vendor. your third party. You know, we always said, "Okay, we're doing third party. We should be doing fourth party. We should probably be thinking about fifth party." And then everyone said, "Oh my god, you know, there's no way." So, it was really a statistical conversation before. We just didn't go there. And now it absolutely is a statistical conversation. It's one of the things that I think is the most frustrating for CISOs is really we're risk managers. I mean that's my job is to statistically tell you what's the likelihood of an incident happening and the potential impact and that should be in your enterprise risk register you know and cyber security have really been often demoted into the lower where the pipes that keep the business going instead of being leveled up and people recognizing that we are key I mean this is how you do business and we need a top level seat top level visib ability and top level conversation with the CEO, you know, with the CTO and with legal and with your your chief risk officer. Those you need to be having serious adult conversations about, okay, what are we trying to accomplish? And then for me to be able to come in and say, okay, you know, here's the likelihood. It's almost like having a financial discussion where or instead of talking about risk as a negative, I'm just saying, okay, you can invest over here or you can invest over here, but if something bad happens over here's the money lost and then we make a decision as a as an organization on where to invest. >> Yeah. And I think you're drawing on a couple of important themes here. I often see in many organizations security being seen as the insurance industry versus being seen as the healthcare or the financial industry. I think that's the difference between having a seale task versus a nonsele task. I mean we kind of paid you to protect us and we'll fire our insurance company if we have to pay liability, right? That kind of a thing. >> Yeah. >> Versus I mean it's like oh there's a traffic policeman and the accident happened. let's fire the police guy and not actually talk to the people who actually made the accident. There's definitely that mindset in the industry and unfortunately that does not bode well for a riskmanagement conversation or this investment conversation that you're talking about. Now the interesting thing that you said was around how everything is always so layered and connected and finding absolute numbers was always hard and with AI I think people are starting with an assumption we'll never be able to find the full attack surface and so on and so forth. specifically talking in the context of securing AI models or AI enabled workflows but I have a little bit of a different opinion. I think when every threat vector or every threat item we have had this moment where everything looked so big in the beginning nothing was fully calculable like I remember year 2000 I was working in the first generation of static source code analysis companies the goal was to find vulnerabilities >> right and I'm just fresh out of college at that time and I'm looking at this problem and going like what there is so much open source code we are building static code analysis for you know first party code and trying to find crosset scripting and SQL injection in Java web applications that's a tip of the iceberg what about all the libraries that I'm sucking up in first party third party fourth party so at that point I came to the same conclusion like ah this is not necessarily a game of pure number this is a game of health management some broad indices right and then same thing happened with first party security second party security third party security the world is super connected Right. And then these layers will continue to be impacting each other. It's the appetite that you have which is the elevation conversation that you talk about. We have to be able to say I can only secure y much in my parimeter. After that I have to take some calls in terms of how far I'm willing to go in that regard. And that's the investment versus risk calculation. So I kind of relate to what you said there to some extent. Chan, do you have more? >> Yeah, I would love to double click into the SPLX product. You guys have a really impressive product suite that protects your customers against all of these risks. What are the most exciting things that you like about the product and how does it really help customers in this world? So, because I was part of the OASP top 10 for LLM project, every person who had an AI security idea was reaching out to me asking me to look at what they were developing and give feedback on it. And I always call myself Sandy the dream killer because people would show me their product and I'd be like, "Yeah, no. I see that you have a really good idea and you're super smart, but you don't have product market fit as a CISO. I would never look at this. You haven't solved my problem." When Chris, the CEO of SPLX, reached out to me, it was the first time that I went, "Oh my gosh, he gets it. He finally understands the pain that CISOs are in, which is really visibility. I don't necessarily need to know all of the nuances, although we're able to do that to you in detail, but the platform had mapped to all of the common risk frameworks. If my executives came to me and said, "Hey, you know, tell me what we're doing around AI security and safety," I could have an answer within 30 seconds. And so that was well over a year ago as the product evolved. Initially at that time when Chris first talked to me guard rails you know everyone was leading with guardrails and as a person who who was an active user I found guard rails very frustrating because I was busy trying to use these tools and I kept getting blocked and at times I was going out to hugging face toace go find a model that didn't have anything on it and so from my own experience I knew that there was a friction point with guardrails and it was there was a balance between putting enough guard rails in place so that the really bad stuff was kept out but not necessarily causing too much friction for your users. There's no sense in having a tool if people can't use it. And so as the as SPLX has evolved, we start out with AI red teaming. Some of the absolute best AI red teamers, you know, you and again developed out of our own pain, which is our team was being asked to go out and do all of this AI red teaming. They were like this manually doing all of this prompting is intensive and timeconuming and kind of boring. There's got to be a way to automate it. So that was where the initial idea came from was automating our own testing. We have some of the top AI red teamers in the world that are part of our organizations and then recognizing that you could reduce being domain specific. So okay, creating attacks that are specific. If you're testing a healthc care chatbot, it'll be different from somebody who's in Qatar was another country. You know, bias in Qar is different than bias in the United States. And understanding that context was king. And so to actually be able to do good AI red teaming, you it was important to be automated, but you had to automate it within context. And so we now have the guardrails logging, being able to monitor both in runtime as well as being able to see the logs after an event. So then again, you can take all of that knowledge and actually translate that into your SIM. So if you're seeing any kind of automated attacks against your system, you can flag on it. So it was really trying to understand the challenge of of actually enabling AI which I'm definitely very pro AI but how do I do that and not cause you more harm to my organization? >> What are some of the insights that you guys get that you feel that your competitors just don't quite see yet? I would say that the multi-turn attacks, here's an interesting example. One of our customers came to us and said, "Okay, we're deploying this internally. We want to make sure that our chatbot doesn't ever disclose another conversation to somebody else." So, we tested it, but it we had to test it 200 times. So that's another unique aspect of AI which in before if you were doing QA testing you had okay here's here's what we tested here's an expected outcome and as soon as you got your expected as outcome you pass the test anything that wasn't the expected outcome failed. So with non-deterministic systems you can run the same pro prompt five times 10 times in this case we ran it 200 times. So for an organization again you're back to that statistical analysis. How many times you are you going to run a prompt before you say okay we've passed that we've met our expectations around safety and security. >> Yeah. Yeah. So I think you mentioned visibility AI red teaming automating AI red teaming and some of the thoughts that I've been having recently around the deterministic and nondeterministic nature as well as this spread that you get with a system like this is like you know we're all aware of how networking ports work and how firewalls work. So you have 0 to8500 and the goal here is to figure out which of the range is now currently open and do the recon on that and then ultimately write a firewall rule to plug that one if it's not a valid business case. Right? So it's like somebody just did not configure it right. You're just finding the punch the hole into it. The multit attacks and so on and so forth. It's kind of like that. You keep probing until you find and it's just still unclear how much probe you do because the range is not fixed. It's not 0 to 8500. the range is so large like you just have to time bound your assessment almost to the point where I can say hm that's how much somebody would go after if I was business value blah and so I need to limit my things how do you balance this need for testing and the amount of effort you put in to find the perfect balance of visibility versus patching your thing versus detecting in runtime when your guardrail was not sufficient like what's the current thought process >> so for many of the successful attacks if you look at plying the prompter or anything from the BT6 teams like the props that they're doing they're I mean they're pretty heavy I mean they're do they're doing they're saying and doing a lot of things to actually get the responses that they want again none of this is simple but they're using obscure languages they're using they're doing conversion they're putting it into images and so that's why the runtime is so important Because if you see someone trying to prompt in Navajo, you know, that's not a normal behavior. And so you can flag on that. It's finding tuning for again keeping out the known bad. You know, if it's a healthcare chatbot, it shouldn't be asking for poetry from the tutor dynasty. You're identifying what looks stupid or what is an unexpected request and then identifying so blocking out the worst of it. But then if you see any unusual activity, it's just like tuning any behavior analytics shish. And I think it's hard though. I mean it's that has been the nuance of trying to do any analytics is what is normal behavior as a CISO within a healthcare company. I used to always try to do research. I got blocked all the time, you know, within my organization because I was trying to look at malicious attacks and they're saying, "Sorry, you can't look that up." Okay. Well, I need to look it up. >> How do I do that? And so I it's finding that balance between what's normal behavior and what's abnormal behavior. And I think about this a lot is huge because, you know, in some ways organizations still have that moat um mentality where we're going to keep all of the bad stuff out. We're going to control. We're not going to let anybody use AI. But, you know, it may be time to really rethink how you do your networking. Maybe you put more emphasis on what are the crown jewels, how do we lock down what's mission critical and really control access and maybe free up provide less friction around areas. It really is a call to uh redefine we talk about zero trust. We all know the challenges around zero trust but part of it was people kept trying to do zero trust on this you know these ancient infrastructures that they had data everywhere. They didn't have a data catalog. they hadn't done any of the maturity steps that we've been preaching about in the past. And so it maybe maybe the move to AI is another reason why you should go back and do some of that cleanup and restructuring of your network and your segmentation and understanding where are the crown jewels. I mean as much as we talk about destroying data, who actually does it? You know, who actually has a program in place where they actively destroy old data? I certainly haven't run across anybody who has a great program around that. >> Iron Mountain still has a business though. So I think there's some users somewhere. >> I think I relate to your your narrative quite a bit. In summary, I think the identify, prevent, detect, respond spectrum has stood the test of time and it's no different for AI security. We still have to identify visibility, prevent, patch by testing, you know, >> resilience. Yeah. >> Exactly. So that that entire spectrum applies. NIST original framework is still very is standing the test of time in that regard. Right. You're absolutely right about legacy stuff makes it harder to implement zero trust principles and newer technology equally bad at making this happen because there's always a shifting ground, right? Zero trust can only be built on static things, not moving targets like AI in that regard, right? We don't know what GPD6 is going to do in terms of its capabilities or its guardrail reduction and then how people will jockey around it to make their own changes and what have you. So yeah, it's definitely a fertile target in that sense enough to keep all the security people on their toes in general. Um, but moving a little bit into what you had alluded earlier about how AI security is a statistics problem and so on and so forth. I think we have traditionally as security practitioners struggled sometimes to communicate with the board on the health of the company or the risk to the company. There's so many ways to describe the same thing and it's all related to what your board understands security to be. Now AI security is not making it easy in my opinion to communicate the risk back up to the board. So from a pure governance and risk management standpoint, what advice do you have for practitioners to say these are the metrics you should follow and this is how you should communicate AI security and AI risk back to the board. Maybe some light on that would be great. >> Yeah, I would say that what you want to give to your board is that I call it the smoky the bear sign. You know, what's the likely the chance of fire? You want it that simple. And then you know what I their decision back to me is we either accept the risk this was you know this is within our risk tolerance and we accept this risk or we want to invest. And I think those AI is actually helping us have those conversations because I compare traditional it to we were using sticks. We were defending with sticks and our adversaries were using sticks and they were beating us with our sticks and we were beating them back with our sticks. And now we have this kind of nape thrower where the size of of the impact can be so um huge that you know we we really have to have very serious conversations with our board about the potential damage. If you look at the proposed law from within California, SB53 has been gotten quite a bit of attention, but they also have one around specifically around chat bots for minors. And the fines for that are like $1,000 per incident that they find. and and the requirements are you have if you have a chatbot and they're talking about character AI chat GBT I think falls within scope the those fines for that and you actually have to report to their suicide prevention line annually and provide data and all of this analysis and things but the cost you know like if you're found if they come decide to find you for that's those are pretty heavy fines that as an organization you definitely want to take that into consideration and you probably want to pri prioritize it based on impact. The key is and for every organization that has to do ISO sock 2, HIPPA, you know, PCI, you you have to centralize those controls. Make sure that you're that that you have some consistency so that you're not just finding yourself just being, you know, not able to do security, you're just running around trying to do audits all of the time. So, >> thank you so much. That's super super interesting and this segus far very well into our last question which is if you could give enterprises one piece of advice on surviving this AI transform threat landscape what would it be and why? Recognize that the biggest change with AI is speed. you no longer can move at a snail's pace to make decisions or you know um look at the pro look at every process across your organization and say how do we do this more quickly how do we move at the speed of AI John Boyd created the udaloop and the reason he did it it was created because he was a fighter pilot and he knew that when you're in a dog fight in the sky you had to the person who could make a decision the fastest was and most often was the winner of the fight. And so being able to make decisions and then you know move forward, make a decision, you know, observe, orient, decide and act and keep following that loop and just re recognize that speed is the most important thing that you need to be able to evolve into this AI world. >> Thank you so much, Sandy. This has been an incredible conversation and thank you for sharing all of your words of wisdom. Thank you everyone for listening to the Def podcast.